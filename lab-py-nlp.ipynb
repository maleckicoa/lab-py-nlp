{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf31a39",
   "metadata": {},
   "source": [
    "# **Comprehensive NLP Lab: From Preprocessing to Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5fcd9",
   "metadata": {},
   "source": [
    "In this lab, you will explore a wide range of Natural Language Processing (NLP) techniques, from basic text preprocessing to advanced feature extraction and analysis. By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Tokenize** and preprocess text data.\n",
    "2. Remove **stop words** and **punctuation**.\n",
    "3. Apply **stemming** and **lemmatization**.\n",
    "4. Extract features using **Bag of Words (BoW)** and **TF-IDF**.\n",
    "5. Generate **n-grams** to capture contextual information.\n",
    "6. Evaluate the impact of different preprocessing techniques on text data.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdb53e",
   "metadata": {},
   "source": [
    "## **1. Setup the Environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa12605",
   "metadata": {},
   "source": [
    "Before we begin, ensure you have the necessary libraries installed. Run the following cell to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839e6cf",
   "metadata": {},
   "source": [
    "Now, import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27aa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a2544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec67490",
   "metadata": {},
   "source": [
    "## **2. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6877c3",
   "metadata": {},
   "source": [
    "### **Exercise 1: Tokenization and Stop Word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bdaa1",
   "metadata": {},
   "source": [
    "Tokenize the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d82fcea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " 'of',\n",
       " 'study',\n",
       " '!',\n",
       " 'it',\n",
       " 'involves',\n",
       " 'analyzing',\n",
       " 'and',\n",
       " 'understanding',\n",
       " 'human',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "# your code here\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26303ab9",
   "metadata": {},
   "source": [
    "Remove stop words and store the result in a variable called `filtered_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d09992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# your code here\n",
    "filtered_tokens = set(tokens) - stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14120510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: {')', 'processing', 'analyzing', 'natural', 'language', 'study', 'understanding', 'involves', 'nlp', 'fascinating', 'human', '.', '!', 'field', '('}\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0b30d",
   "metadata": {},
   "source": [
    "### **Exercise 2: Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6949309",
   "metadata": {},
   "source": [
    "Apply stemming and lemmatization to the `filtered_tokens`. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03dd22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063d90e",
   "metadata": {},
   "source": [
    "Apply stemming and store the result in `stemmed_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6120c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = set()\n",
    "for token in filtered_tokens:\n",
    "    stemmed_token = stemmer.stem((token))\n",
    "    stemmed_tokens.add(stemmed_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2c5d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: {')', 'analyz', 'involv', 'process', 'fascin', 'languag', 'understand', 'natur', '!', 'field', 'nlp', 'human', '.', 'studi', '('}\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc333a",
   "metadata": {},
   "source": [
    "Apply lemmatization and store the result in `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2b23234",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = set()\n",
    "for token in stemmed_tokens:\n",
    "    lemmatized_token = lemmatizer.lemmatize((token))\n",
    "    lemmatized_tokens.add(lemmatized_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f49cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: {')', 'analyz', 'involv', 'process', 'languag', 'understand', 'natur', '!', 'field', 'nlp', 'human', '.', 'fascin', 'studi', '('}\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03b8b9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyz',\n",
       " 'fascin',\n",
       " 'field',\n",
       " 'human',\n",
       " 'involv',\n",
       " 'languag',\n",
       " 'natur',\n",
       " 'nlp',\n",
       " 'process',\n",
       " 'studi',\n",
       " 'understand'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "lemmatized_tokens =  lemmatized_tokens - punct\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20531b9b",
   "metadata": {},
   "source": [
    "## **3. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbd840",
   "metadata": {},
   "source": [
    "### **Exercise 3: Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7765074",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` from `scikit-learn` to create a Bag of Words representation of the following corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c86f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP.\",\n",
    "    \"NLP is amazing.\",\n",
    "    \"I enjoy learning new things in NLP.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45334917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your code here\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a BoW representation\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfb3b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 1 1 1]]\n",
      "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d79220",
   "metadata": {},
   "source": [
    "### **Exercise 4: TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55622108",
   "metadata": {},
   "source": [
    "Use the `TfidfVectorizer` from `scikit-learn` to create a TF-IDF representation of the same corpus. Store the result in `X_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba20ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a TF-IDF representation\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0df3ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      " [[0.         0.         0.         0.         0.         0.861037\n",
      "  0.         0.50854232 0.        ]\n",
      " [0.65249088 0.         0.         0.65249088 0.         0.\n",
      "  0.         0.38537163 0.        ]\n",
      " [0.         0.43238509 0.43238509 0.         0.43238509 0.\n",
      "  0.43238509 0.2553736  0.43238509]]\n",
      "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6cee3",
   "metadata": {},
   "source": [
    "### **Exercise 5: N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9d66d",
   "metadata": {},
   "source": [
    "Generate `bigrams (2-grams)` from the corpus using `CountVectorizer`. Store the result in `X_bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9de7b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Initialize the CountVectorizer with ngram_range=(2, 2)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "# Step 2: Fit and transform the corpus into a bigram representation\n",
    "X_bigram = bigram_vectorizer.fit_transform(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b210fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      " [[0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0]\n",
      " [1 1 0 1 0 1 0 1]]\n",
      "Bigram Vocabulary: ['enjoy learning' 'in nlp' 'is amazing' 'learning new' 'love nlp'\n",
      " 'new things' 'nlp is' 'things in']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigrams:\\n\", X_bigram.toarray())\n",
    "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7896acb",
   "metadata": {},
   "source": [
    "## **4. Advanced Exercise: Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68072a1e",
   "metadata": {},
   "source": [
    "### **Exercise 6: Build a Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb33268",
   "metadata": {},
   "source": [
    "Combine all the preprocessing steps (tokenization, stop word removal, punctuation removal, stemming/lemmatization) into a single function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    \n",
    "    # Step 1: Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Step 2: Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = set(tokens) - stop_words\n",
    "\n",
    "    # Step 3: Remove punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    clean_tokens =  filtered_tokens - punct\n",
    "\n",
    "    # Step 4: Apply lemmatization\n",
    "    lemmatized_tokens = set()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token in clean_tokens:\n",
    "        lemmatized_token = lemmatizer.lemmatize((token))\n",
    "        lemmatized_tokens.add(lemmatized_token)\n",
    "\n",
    "   \n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8170a6c",
   "metadata": {},
   "source": [
    "Apply this function to the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1777799",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "\n",
    "processed_text = text_preprocessing_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c75b50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: {'processing', 'analyzing', 'natural', 'language', 'study', 'understanding', 'involves', 'nlp', 'fascinating', 'human', 'field'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625df20",
   "metadata": {},
   "source": [
    "## **5. Evaluation of Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a666da0",
   "metadata": {},
   "source": [
    "### **Exercise 7: Compare Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05a37",
   "metadata": {},
   "source": [
    "Compare the results of stemming and lemmatization on the following sentence. Store the results in `stemmed_tokens` and `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e70cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cats are playing with the mice in the garden.\"\n",
    "\n",
    "# Step 1: Tokenize and preprocess the sentence and store the result in filtered_tokens\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence.lower())\n",
    "filtered_tokens = set(tokens) - set(stopwords.words('english'))\n",
    "filtered_tokens =  filtered_tokens - set(string.punctuation)\n",
    "\n",
    "stemmed_tokens = set()\n",
    "stemmer = PorterStemmer()\n",
    "for token in filtered_tokens:\n",
    "    stemmed_token = stemmer.stem((token))\n",
    "    stemmed_tokens.add(stemmed_token)\n",
    "\n",
    "lemmatized_tokens = set()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for token in filtered_tokens:\n",
    "    lemmatized_token = lemmatizer.lemmatize((token))\n",
    "    lemmatized_tokens.add(lemmatized_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "125ca6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: {'mice', 'playing', 'cats', 'garden'}\n",
      "Stemmed Tokens: {'mice', 'garden', 'cat', 'play'}\n",
      "Lemmatized Tokens: {'playing', 'mouse', 'garden', 'cat'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c465f",
   "metadata": {},
   "source": [
    "## **6. Real-World Dataset: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a598d9",
   "metadata": {},
   "source": [
    "### **Exercise 8: Preprocess and Analyze Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056ab80",
   "metadata": {},
   "source": [
    "In this exercise, you will work with a real-world dataset of tweets. The dataset contains 5000 positive and 5000 negative tweets. Your task is to preprocess the tweets and extract features for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1e37f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/aleksamihajlovic/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2c60819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43847ae",
   "metadata": {},
   "source": [
    "Load the dataset of positive and negative tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b423ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b8248",
   "metadata": {},
   "source": [
    "Combine them into a single list called ``all_tweets`` and create a corresponding list of labels called `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "labels = [1]* len(positive_tweets) + [0]* len(positive_tweets)\n",
    "# Combine the datasets\n",
    "all_tweets = positive_tweets + negative_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32ec3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Print a sample tweet\n",
    "print(\"Sample Tweet:\", all_tweets[0])\n",
    "print(\"Label:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a04a",
   "metadata": {},
   "source": [
    "### **Exercise 9: Preprocess Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1e984",
   "metadata": {},
   "source": [
    "Apply the custom preprocessing pipeline to the entire dataset of tweets. Store the result in ``preprocessed_tweets``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39e8c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply the preprocessing pipeline to all tweets\n",
    "# your code here\n",
    "preprocessed_tweets = [\" \".join(text_preprocessing_pipeline(text)) for text in all_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edeef254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Tweets Sample: milipol_paris week engaged member top community followfriday pkuchly57 france_inte\n"
     ]
    }
   ],
   "source": [
    "# Print a sample preprocessed tweet\n",
    "print(\"Preprocessed Tweets Sample:\", preprocessed_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8658daf",
   "metadata": {},
   "source": [
    "### **Exercise 10: Feature Extraction on Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f161c",
   "metadata": {},
   "source": [
    "Extract features from the preprocessed tweets using **Bag of Words** and **TF-IDF**. Store the results in ``X_bow`` and ``X_tfidf``, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee42f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Vocabulary: ['000' '10' '100' '11' '12' '13' '15' '20' '2015' '24' '2nd' '30' '40'\n",
      " '50' '5sos' '969horan696' 'able' 'absolutely' 'acc' 'account' 'act'\n",
      " 'actually' 'add' 'added' 'address' 'adeccowaytowork' 'af' 'afternoon'\n",
      " 'age' 'ago' 'agree' 'ah' 'ahh' 'ai' 'aint' 'air' 'airport' 'al' 'album'\n",
      " 'all' 'allah' 'almost' 'alone' 'along' 'already' 'alright' 'also'\n",
      " 'always' 'amazing' 'amber' 'amp' 'ang' 'annoying' 'another' 'answer'\n",
      " 'anymore' 'anyone' 'anything' 'anyway' 'app' 'apparently' 'appreciate'\n",
      " 'appreciated' 'aqui' 'around' 'arrived' 'art' 'article' 'as' 'ask'\n",
      " 'asked' 'asking' 'asleep' 'ate' 'august' 'australia' 'available' 'aw'\n",
      " 'awake' 'away' 'awesome' 'awful' 'aww' 'awww' 'awwww' 'babe' 'baby'\n",
      " 'back' 'bad' 'badly' 'bae' 'bag' 'ball' 'bam' 'bank' 'barsandmelody' 'bb'\n",
      " 'bby' 'bc' 'bday' 'beach' 'beat' 'beautiful' 'become' 'bed' 'beli'\n",
      " 'believe' 'best' 'bestfriend' 'bet' 'better' 'bhaktisbanter' 'big' 'bill'\n",
      " 'birthday' 'bit' 'bitch' 'black' 'blog' 'blue' 'body' 'book' 'bored'\n",
      " 'bought' 'box' 'boy' 'brain' 'braindots' 'brand' 'bravefrontiergl'\n",
      " 'break' 'breakfast' 'breaking' 'brilliant' 'bring' 'bro' 'broke' 'broken'\n",
      " 'brother' 'btw' 'buddy' 'bus' 'business' 'busy' 'buy' 'bye' 'ca' 'cake'\n",
      " 'call' 'called' 'came' 'cant' 'car' 'card' 'care' 'carterreynolds' 'case'\n",
      " 'cat' 'catch' 'cause' 'cc' 'ceo1month' 'certain' 'chance' 'change'\n",
      " 'channel' 'chat' 'check' 'cheer' 'cheese' 'chicken' 'child' 'chill'\n",
      " 'chocolate' 'choice' 'choiceinternationalartist' 'chris' 'city' 'class'\n",
      " 'click' 'close' 'club' 'co' 'code' 'coffee' 'cold' 'collection' 'come'\n",
      " 'coming' 'comment' 'community' 'completely' 'computer' 'concert'\n",
      " 'confused' 'congrats' 'congratulation' 'connect' 'contact' 'conversation'\n",
      " 'cool' 'could' 'count' 'country' 'couple' 'course' 'crazy' 'cream' 'cry'\n",
      " 'currently' 'cut' 'cute' 'da' 'dad' 'damn' 'dark' 'date' 'day' 'dead'\n",
      " 'dear' 'definitely' 'deserve' 'design' 'detail' 'didnt' 'die' 'different'\n",
      " 'dinner' 'dm' 'dog' 'done' 'dont' 'door' 'dot' 'download' 'dream' 'dress'\n",
      " 'drink' 'drive' 'driving' 'drop' 'dude' 'due' 'dying' 'ear' 'earlier'\n",
      " 'early' 'easy' 'eat' 'eating' 'either' 'else' 'email' 'emilybett' 'end'\n",
      " 'ended' 'england' 'english' 'enjoy' 'enjoyed' 'enjoying' 'enough'\n",
      " 'episode' 'especially' 'eve' 'even' 'event' 'ever' 'every' 'everyday'\n",
      " 'everyone' 'everything' 'exactly' 'exam' 'excited' 'exciting'\n",
      " 'ext098yq1b' 'extra' 'eye' 'fab' 'face' 'facebook' 'fact' 'failed' 'fair'\n",
      " 'fall' 'family' 'fan' 'fantastic' 'far' 'fast' 'fav' 'favorite'\n",
      " 'favourite' 'fb' 'fback' 'feedback' 'feel' 'feeling' 'fell' 'felt'\n",
      " 'festival' 'fever' 'ff' 'final' 'finally' 'find' 'fine' 'finger' 'finish'\n",
      " 'finished' 'first' 'fix' 'flipkartfashionfriday' 'fly' 'folk' 'follback'\n",
      " 'follow' 'followback' 'followed' 'follower' 'followfriday' 'following'\n",
      " 'food' 'foot' 'forever' 'forget' 'forgot' 'forward' 'found' 'four' 'free'\n",
      " 'french' 'friday' 'friend' 'front' 'fuck' 'fucked' 'fucking' 'full' 'fun'\n",
      " 'funny' 'future' 'game' 'garden' 'gave' 'gay' 'get' 'getting' 'gift'\n",
      " 'girl' 'give' 'giveaway' 'giving' 'glad' 'glass' 'go' 'god' 'going'\n",
      " 'gold' 'gon' 'gone' 'good' 'goodbye' 'goodmorning' 'goodnight' 'google'\n",
      " 'gorgeous' 'got' 'great' 'green' 'group' 'gt' 'guess' 'gutted' 'guy'\n",
      " 'gym' 'ha' 'hacked' 'haha' 'hahaha' 'hahahaha' 'hai' 'hair' 'half' 'hand'\n",
      " 'happen' 'happened' 'happens' 'happiness' 'happy' 'hard' 'harry' 'hate'\n",
      " 'havent' 'hay' 'he' 'head' 'hear' 'heard' 'heart' 'hehe' 'hell' 'hello'\n",
      " 'help' 'hey' 'hi' 'high' 'hit' 'hold' 'holiday' 'home' 'hope' 'hopefully'\n",
      " 'hoping' 'hornykik' 'horrible' 'hot' 'hotel' 'hour' 'house' 'http' 'hug'\n",
      " 'huge' 'huhu' 'hungry' 'hurry' 'hurt' 'ice' 'id' 'idea' 'idk' 'ignore'\n",
      " 'ill' 'im' 'impastel' 'indeed' 'indiemusic' 'infinite' 'influencers'\n",
      " 'info' 'inside' 'instagram' 'instead' 'interested' 'interesting'\n",
      " 'internet' 'invite' 'iphone' 'isnt' 'issue' 'ive' 'james' 'jaymcguiness'\n",
      " 'jealous' 'jnlazts' 'job' 'join' 'july' 'justi' 'justinbieber' 'ka'\n",
      " 'keep' 'kid' 'kidding' 'kik' 'kikchat' 'kikgirl' 'kikhorny' 'kikme'\n",
      " 'kikmeboys' 'kikmeguys' 'kikmenow' 'kiksex' 'kiksexting' 'killing' 'kind'\n",
      " 'kinda' 'knew' 'know' 'ko' 'la' 'lady' 'last' 'late' 'later' 'latest'\n",
      " 'law' 'learn' 'least' 'leave' 'leaving' 'leeds' 'left' 'leg' 'less' 'let'\n",
      " 'liam' 'lie' 'life' 'light' 'like' 'liked' 'lil' 'line' 'link' 'list'\n",
      " 'listen' 'listening' 'literally' 'little' 'littlemix' 'live' 'living'\n",
      " 'll' 'lmao' 'load' 'lol' 'london' 'lonely' 'long' 'longer' 'look'\n",
      " 'looked' 'looking' 'lose' 'lost' 'lot' 'love' 'loved' 'lovely' 'low' 'lt'\n",
      " 'luck' 'lucky' 'luke' 'lunch' 'mad' 'made' 'madrid' 'make' 'making' 'man'\n",
      " 'many' 'match' 'mate' 'matter' 'may' 'maybe' 'me' 'mean' 'meant' 'meet'\n",
      " 'meeting' 'member' 'men' 'mention' 'message' 'met' 'middle' 'might' 'min'\n",
      " 'mind' 'mine' 'minute' 'miss' 'missed' 'missing' 'mistake' 'model' 'mom'\n",
      " 'moment' 'monday' 'money' 'month' 'mood' 'morning' 'move' 'movie' 'mr'\n",
      " 'much' 'mum' 'music' 'must' 'na' 'name' 'near' 'need' 'needed' 'never'\n",
      " 'new' 'news' 'next' 'nice' 'night' 'nobody' 'nope' 'nothing' 'notice'\n",
      " 'number' 'offer' 'office' 'oh' 'ohh' 'ok' 'okay' 'old' 'omg' 'one'\n",
      " 'online' 'oops' 'open' 'oppa' 'opportunity' 'order' 'original' 'others'\n",
      " 'otherwise' 'outfit' 'outside' 'pa' 'pack' 'page' 'pain' 'paper' 'parent'\n",
      " 'park' 'part' 'party' 'pas' 'past' 'pay' 'peace' 'people' 'perfect'\n",
      " 'person' 'phone' 'photo' 'pic' 'pick' 'picture' 'place' 'plan' 'planning'\n",
      " 'play' 'playing' 'please' 'pleasure' 'pls' 'plus' 'plz' 'point' 'pool'\n",
      " 'poor' 'positive' 'possible' 'post' 'power' 'pray' 'pre' 'present'\n",
      " 'pretty' 'pro' 'probably' 'problem' 'product' 'project' 'proud' 'put'\n",
      " 'queen' 'question' 'quick' 'quite' 'quote' 'radio' 'rain' 'raining'\n",
      " 'rcvcyyo0iq' 're' 'read' 'reading' 'ready' 'real' 'real_liam_payne'\n",
      " 'really' 'reason' 'received' 'red' 'release' 'remember' 'reply' 'rest'\n",
      " 'retweet' 'review' 'right' 'rip' 'rn' 'rock' 'room' 'rt' 'rude' 'run'\n",
      " 'running' 'sa' 'sad' 'sadly' 'safe' 'said' 'sale' 'saturday' 'save' 'saw'\n",
      " 'say' 'saying' 'scared' 'schedule' 'school' 'season' 'second' 'secret'\n",
      " 'see' 'seeing' 'seems' 'seen' 'selenagomez' 'selfie' 'send' 'sending'\n",
      " 'sent' 'series' 'serious' 'seriously' 'service' 'session' 'set' 'sex'\n",
      " 'sexy' 'shame' 'share' 'sharing' 'shift' 'ship' 'shit' 'shop' 'shopping'\n",
      " 'short' 'shot' 'shout' 'show' 'sick' 'side' 'sigh' 'sign' 'since'\n",
      " 'single' 'sir' 'sister' 'site' 'size' 'skype' 'sleep' 'sleeping' 'slept'\n",
      " 'slow' 'small' 'smile' 'smiling' 'snapchat' 'solo' 'someone' 'something'\n",
      " 'sometimes' 'somewhere' 'song' 'soon' 'sooo' 'sore' 'sorry' 'sort' 'soul'\n",
      " 'sound' 'special' 'spend' 'stage' 'stand' 'star' 'start' 'started'\n",
      " 'stats' 'stay' 'still' 'stomach' 'stop' 'stopped' 'store' 'story'\n",
      " 'stream' 'stress' 'strong' 'stuck' 'student' 'stuff' 'stupid' 'style'\n",
      " 'suck' 'summer' 'sun' 'sunday' 'sunshine' 'super' 'superjunior' 'support'\n",
      " 'supporting' 'supposed' 'sure' 'surprise' 'swear' 'sweet' 'ta'\n",
      " 'tagsforlikes' 'take' 'taken' 'taking' 'talk' 'talking' 'tbh' 'tcot'\n",
      " 'tea' 'team' 'teen' 'teenchoice' 'tell' 'terrible' 'test' 'text' 'tgif'\n",
      " 'thank' 'thanks' 'thankyou' 'thats' 'thing' 'think' 'thinking' 'tho'\n",
      " 'though' 'thought' 'three' 'throat' 'thx' 'ticket' 'till' 'time' 'timing'\n",
      " 'tired' 'tl' 'today' 'together' 'tolajobjobs' 'told' 'tom' 'tomorrow'\n",
      " 'tonight' 'took' 'top' 'totally' 'touch' 'tour' 'town' 'train' 'training'\n",
      " 'travel' 'treat' 'tried' 'trip' 'true' 'truly' 'try' 'trying' 'tuesday'\n",
      " 'turn' 'turned' 'tv' 'tweet' 'tweeting' 'twitch' 'twitter' 'two' 'ty'\n",
      " 'uber' 'ubericecream' 'uberuk' 'ugh' 'ugly' 'uk' 'understand'\n",
      " 'unfollowers' 'unfortunately' 'uniteblue' 'update' 'upset' 'ur' 'use'\n",
      " 'used' 'using' 'usually' 'uzoaqrowkx' 'vacation' 've' 'version' 'via'\n",
      " 'vid' 'vidcon' 'video' 'view' 'visit' 'voice' 'vote' 'wait' 'waiting'\n",
      " 'wake' 'waking' 'walk' 'wall' 'wan' 'want' 'wanted' 'warm' 'warsaw'\n",
      " 'waste' 'watch' 'watched' 'watching' 'water' 'way' 'wear' 'weather'\n",
      " 'website' 'wedding' 'week' 'weekend' 'weird' 'welcome' 'well' 'went'\n",
      " 'wet' 'wforwoman' 'whats' 'white' 'whole' 'wi' 'wicked' 'wife' 'win'\n",
      " 'window' 'wish' 'wishing' 'without' 'wo' 'woke' 'woman' 'wonder'\n",
      " 'wonderful' 'wont' 'word' 'work' 'working' 'world' 'worry' 'worse'\n",
      " 'worst' 'worth' 'would' 'wow' 'write' 'writing' 'wrong' 'wsalelove' 'wtf'\n",
      " 'x15' 'xd' 'xx' 'xxx' 'ya' 'yay' 'yeah' 'year' 'yep' 'yes' 'yesterday'\n",
      " 'yet' 'yo' 'youre' 'youth' 'youtube' 'yup' 'zayn'\n",
      " 'zayniscomingbackonjuly26' 'zaynmalik' 'ｍｅ' 'ｓｅｅ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Create a Bag of Words representation\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(preprocessed_tweets)\n",
    "#print(\"Bag of Words:\\n\", X.toarray())\n",
    "#print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Step 2: Create a TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "#print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
    "#print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912687c",
   "metadata": {},
   "source": [
    "## **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ed4ac",
   "metadata": {},
   "source": [
    "In this lab, you explored a wide range of NLP techniques, from basic text preprocessing to advanced feature extraction and analysis. You also worked with a real-world dataset of tweets and applied your knowledge to preprocess and extract features for sentiment analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
